# awesome-text-to-3D




2023:
- Li et al., "3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion," arXiv, 2023.
- Hong et al., "Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation," arXiv, 2023.
- Kim et al., "PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion," arXiv, 2023.
- Singer et al., [Text-to-4d dynamic scene generation](https://arxiv.org/abs/2301.11280), arXiv,2023.
- Liu et al., [Zero-1-to-3: Zero-shot One Image to 3D Object](https://arxiv.org/abs/2303.11328), arXiv, 2023.
- Seo et al., [Let 2D Diffusion Model Know 3D-Consistency for Robust Text-to-3D Generation](https://arxiv.org/abs/2303.07937), arXiv, 2023.
- Tang et al., [Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior](https://make-it-3d.github.io/), arXiv, 2023.
- Zhang et al.,[Text-to-image Diffusion Models in Generative AI: A Survey](https://arxiv.org/abs/2303.07909), arXiv, 2023.
- Lin et al., [Magic3D: High-Resolution Text-to-3D Content Creation](https://research.nvidia.com/labs/dir/magic3d/), CVPR, 2023.
- Poole et al., [DreamFusion: Text-to-3D using 2D Diffusion](https://dreamfusion3d.github.io/), ICLR, 2023.
- Richardson et al., [TEXTure: Text-Guided Texturing of 3D Shapes](https://arxiv.org/pdf/2302.01721.pdf), arXiv, 2023.
- Haque et al., [Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions](https://arxiv.org/pdf/2303.12789.pdf), arXiv, 2023.
- Jun and Nichol, [ShapÂ·E: Generating Conditional 3D Implicit Functions](https://github.com/openai/shap-e), arXiv, 2023.
- Chen et al., [Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation](https://arxiv.org/abs/2303.13873), arXiv, 2023.



